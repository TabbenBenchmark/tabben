{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b709a520",
   "metadata": {},
   "source": [
    "+++\n",
    "title = \"Working with (Vanilla) PyTorch\"\n",
    "+++\n",
    "\n",
    "This guide goes through how to use this package with the standard PyTorch workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ef7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tabben torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092b846",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "For this example, we'll use the poker hand prediction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc79f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from `https://github.com/umd-otb/OpenTabularDataBenchmark/releases/download/v0.0.5-pre/poker.npz` to `data/poker.npz`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537ff0d514f545898035a1af9387937b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6867096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tabben.datasets import OpenTabularDataset\n",
    "\n",
    "ds = OpenTabularDataset('./data/', 'poker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8333441",
   "metadata": {},
   "source": [
    "And let's just look at the input and output attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e7aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Attribute Names:\n",
      "['S1' 'C1' 'S2' 'C2' 'S3' 'C3' 'S4' 'C4' 'S5' 'C5']\n",
      "Output Attribute Names:\n",
      "['label']\n"
     ]
    }
   ],
   "source": [
    "print('Input Attribute Names:')\n",
    "print(ds.input_attributes)\n",
    "\n",
    "print('Output Attribute Names:')\n",
    "print(ds.output_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ceb2a",
   "metadata": {},
   "source": [
    "Since we're working with PyTorch, the `OpenTabularDataset` object above is a PyTorch `Dataset` object that we can directly feed into a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5939e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape (Batched): torch.Size([8, 10])\n",
      "Output Shape (Batched): torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(ds, batch_size=8)\n",
    "\n",
    "example_batch = next(iter(dl))\n",
    "print(f'Input Shape (Batched): {example_batch[0].shape}')\n",
    "print(f'Output Shape (Batched): {example_batch[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c04c96",
   "metadata": {},
   "source": [
    "## Setting up a basic model\n",
    "\n",
    "First, we'll create a basic model in PyTorch, just for illustration (you can replace this with whatever model you're trying to train/evaluate). It'll just be a feedforward neural network with a couple dense/linear layers (this probably won't perform well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb43bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ShallowClassificationNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, 32)\n",
    "        self.linear2 = nn.Linear(32, 32)\n",
    "        self.linear3 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # [b, num_inputs] -> [b, 32]\n",
    "        x = F.relu(self.linear1(inputs))\n",
    "        \n",
    "        # [b, 32] -> [b, 32]\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        # [b, 32] -> [b, num_classes] (log(softmax(.)) computed over each row)\n",
    "        x = F.log_softmax(self.linear3(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        x = F.relu(self.linear1(inputs))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return F.softmax(self.linear3(x), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d542a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')  # change this to 'cuda' if you have access to a CUDA GPU\n",
    "model = ShallowClassificationNetwork(ds.num_inputs, ds.num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0305c",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we have a basic model and a training dataset (the default split for `OpenTabularDataset` is the train split), we can train our simple network using a PyTorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0783d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316f9c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 553.88it/s, train loss=0.0089, train acc=100.00%]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "model.train()\n",
    "\n",
    "training_progress = trange(30, desc='Train epoch')\n",
    "for epoch in training_progress:\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    running_count = 0\n",
    "    \n",
    "    for batch_input, batch_output in dl:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch_input.float().to(device))\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        \n",
    "        loss = criterion(outputs, batch_output)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += (preds == batch_output).sum()\n",
    "        running_count += batch_input.size(0)\n",
    "    \n",
    "    training_progress.set_postfix({\n",
    "        'train loss': f'{running_loss / running_count:.4f}',\n",
    "        'train acc': f'{100 * running_acc / running_count:.2f}%',\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e8466",
   "metadata": {},
   "source": [
    "You can play around with the hyperparameters, but the model isn't likely to get particularly good performance like this. But, we'll go ahead and evaluate the final model (we ignored having a validation set in this guide) on the test set.\n",
    "\n",
    "## Evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29afb611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already available at `data/poker.npz`\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "test_ds = OpenTabularDataset('./data/', 'poker', split='test')\n",
    "test_dl = DataLoader(test_ds, batch_size=16)\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f043703",
   "metadata": {},
   "source": [
    "Let's run the model and save its outputs for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c48acf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "pred_outputs = []\n",
    "gt_outputs = []\n",
    "\n",
    "for test_inputs, test_outputs in test_dl:\n",
    "    batch_outputs = model(test_inputs.float().to(device))\n",
    "    pred_outputs.append(batch_outputs.detach().cpu())\n",
    "    gt_outputs.append(test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f8c59ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5647e+01, -1.6075e+01, -1.5923e+01, -1.5735e+01, -1.5817e+01,\n",
      "         -1.5155e+01, -1.5495e+01, -1.4744e+01, -1.6896e-03, -6.3850e+00],\n",
      "        [-1.0643e+01, -1.0917e+01, -1.0864e+01, -1.0734e+01, -1.0827e+01,\n",
      "         -1.0255e+01, -1.0533e+01, -1.0003e+01, -7.0259e-03, -4.9921e+00],\n",
      "        [-1.0501e+01, -1.0780e+01, -1.0698e+01, -1.0526e+01, -1.0748e+01,\n",
      "         -1.0116e+01, -1.0391e+01, -9.8666e+00, -6.9794e-03, -5.0038e+00],\n",
      "        [-1.3968e+01, -1.4266e+01, -1.4205e+01, -1.4096e+01, -1.4091e+01,\n",
      "         -1.3400e+01, -1.3740e+01, -1.3106e+01, -2.6051e-03, -5.9548e+00],\n",
      "        [-9.1742e+00, -9.8402e+00, -9.8968e+00, -8.9129e+00, -8.7744e+00,\n",
      "         -8.9716e+00, -8.6310e+00, -9.2210e+00, -3.6032e+00, -2.8542e-02],\n",
      "        [-1.5139e+01, -1.6639e+01, -1.6175e+01, -1.4354e+01, -1.4642e+01,\n",
      "         -1.5504e+01, -1.3987e+01, -1.5875e+01, -1.1814e+01, -1.0014e-05],\n",
      "        [-2.1373e+01, -2.3500e+01, -2.2741e+01, -2.0185e+01, -2.0689e+01,\n",
      "         -2.1936e+01, -1.9725e+01, -2.2507e+01, -1.8133e+01,  0.0000e+00],\n",
      "        [-8.4567e+00, -9.1085e+00, -9.1410e+00, -8.2172e+00, -8.1156e+00,\n",
      "         -8.3018e+00, -7.9645e+00, -8.5220e+00, -3.2689e+00, -4.0659e-02],\n",
      "        [-1.1924e+01, -1.2440e+01, -1.2764e+01, -1.1695e+01, -1.1195e+01,\n",
      "         -1.1294e+01, -1.1203e+01, -1.1706e+01, -3.5952e+00, -2.7912e-02],\n",
      "        [-1.3732e+01, -1.5154e+01, -1.4694e+01, -1.3015e+01, -1.3323e+01,\n",
      "         -1.4118e+01, -1.2696e+01, -1.4442e+01, -1.0739e+01, -3.1590e-05]])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "test_pred_outputs = torch.vstack(pred_outputs).detach().cpu()\n",
    "test_gt_outputs = torch.hstack(gt_outputs).detach().cpu()\n",
    "print(test_pred_outputs)\n",
    "print(test_gt_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d9ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "continuous-multioutput format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26117/3399685146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    573\u001b[0m         )\n\u001b[1;32m    574\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         return _average_binary_score(\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_fpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: continuous-multioutput format is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(ds.num_classes)\n",
    "roc_auc_score(test_pred_outputs, test_gt_outputs.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbf4c5",
   "metadata": {},
   "source": [
    "We can get the standard set of metrics and then evaluate the outputs of the test set on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabben.evaluators import get_metrics\n",
    "\n",
    "eval_metrics = get_metrics(ds.task, classes=ds.num_classes)\n",
    "for metric in eval_metrics:\n",
    "    print(metric)\n",
    "    print(f'{metric}: {metric(test_pred_outputs, test_gt_outputs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9038a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This code was last run with the following versions (if you're looking at the no-output webpage, see the notebook in the repository for versions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce28d65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.10.0\n",
      "tabben: 0.0.3.dev0\n",
      "scikit-learn: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "packages = ['torch', 'tabben']\n",
    "\n",
    "for pkg in packages:\n",
    "    print(f'{pkg}: {version(pkg)}')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
